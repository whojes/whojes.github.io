I"(<p>많은 서버에서 여러가지 이유로 중복해서 호출이 일어나는 api 가 있었다. 사실 그러라고 만들어 놓은 api 이기도 했지만, data center squize 테스트를 위해 트래픽을 한 쪽에 몰아 넣은 상황에서 uv 가 50k 정도 되었을 때 문제가 발생했다. 머신난이었던 때 인스턴스를 너무 많이 띄우는 괴물 아니냐는 말과 함께 로직에 최적화가 되어있는게 맞는 것인지 하는 챌린지가 많이 들어왔었다. 그래서, 다방면으로 이유를 분석해서 수정을 진행했다. 효과가 좋았던 것들을 몇개 정리하기로 했다.</p>

<p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_1.png" class="content-image-1" /><br />
  <em>tps 15k</em><br />
</p>

<hr />

<h3 id="redis-cache-사이즈-줄이기"><strong>redis cache 사이즈 줄이기</strong></h3>
<ul>
  <li><strong>why</strong>
    <ul>
      <li>mongo search 속도가 느려 레디스 캐싱을 해두었는데, redis 쪽으로 네트워크 bandwidth가 너무 컸다. 네크워크 비용도 아깝고 언젠가는 문제가 됐을 수 있어서 수정했다.</li>
    </ul>
  </li>
  <li><strong>how</strong>
    <ol>
      <li>시리얼라이저를 바꿈
        <ul>
          <li>레디스 넣기 전에 snappy 를 사용했는데, gzip 으로 바꿨음 <a href="https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats" target="_blank">snappy vs gzip</a></li>
        </ul>
      </li>
      <li>캐싱 데이터를 바꿈
        <ul>
          <li>특정 작업의 결과물을 캐싱하던 것에서, 사용되는 파라미터 중 변동되는 것만 캐싱하고 서버가 작업을 매번 새로 하게 함</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>result</strong>
    <ol>
      <li>두 작업 모두 cpu 를 손해보고 network 를 이득보는 작업이라, 효과는 확실했다. 레디스 인아웃은 <code class="language-plaintext highlighter-rouge">500 MiB/s -&gt; 95 MiB/s</code> 로 감소했지만, api 응답은 p50 <code class="language-plaintext highlighter-rouge">7.5ms -&gt; 12ms</code>, p95 <code class="language-plaintext highlighter-rouge">28ms -&gt; 32ms</code> 로 증가했다.</li>
    </ol>
  </li>
</ul>

<p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_2.png" class="content-image-1" /><br />
  <em>레디스 네트워크 in/out. 첫 드랍은 snappy -&gt; gzip, 두번째 드랍은 캐싱데이터 수정</em><br />
</p>
<p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_3.png" class="content-image-1" /><br />
  <em>수정 후 전날과 응답속도 비교. 연산작업의 최적화가 필요해보인다.</em><br />
</p>

<hr />
<h3 id="mongos-이슈들"><strong>mongos 이슈들</strong></h3>

<ul>
  <li>
    <p><strong>mongos command max response 가 이상한 주기성을 가지고 늘어났음</strong></p>

    <p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_4.png" class="content-image-1" /><br />
  <em>uv 가 높은 상황에서, mongos 별로 주기적으로 max response 가 증가하는 피크가 관측됨</em><br />
</p>

    <ul>
      <li><strong>why</strong>
        <ul>
          <li>mongos 로그 중 <code class="language-plaintext highlighter-rouge">LockBusy: could not acquire collection lock for ${dbName}.${collectionName} to split chunk</code>를 발견 <a href="https://jira.mongodb.org/browse/SERVER-56654" target="_blank">관련 이슈</a></li>
          <li>해당 command 가 날아가는 collection 은 5.7b 개의 다큐먼트를 들고 있고, 인서트도 활발히 일어나는 컬렉션이었다. insert 후에 chunk split 하던 와중 분산락을 획득하는 과정이 있는데, 이게 이슈가 있다고 리포트가 되었음</li>
        </ul>
      </li>
      <li><strong>how</strong>
        <ul>
          <li>mongos 버전을 업데이트하는 것으로 해결.</li>
        </ul>
      </li>
      <li><strong>result</strong>
        <ul>
          <li>해결됨. 그러나…</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>3개의 mongos 중 두개가 갑자기 죽었음</strong></p>

    <ul>
      <li><strong>why</strong>
        <ul>
          <li>mongos 버전을 업데이트 하면서, 메모리 증설을 위해 dc 타겟을 바꾸고 호스트를 변경했었는데, 카나리 배포과정에서 몽고 커넥션 수가 호스트의 pid_max 값을 초과했음</li>
          <li>두개의 mongos 의 pid_max 값이 리눅스 system default 값이었고(32768), 기존 mongos 는 4백만 정도로 세팅을 해서 사용중이었다.
            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@32899a32399f:/# <span class="nb">cat</span> /proc/sys/kernel/pid_max
32768
</code></pre></div>            </div>
          </li>
          <li>참으로 운이 좋게 같은 세팅값의 세번째 mongos 는 살아있어서, mongos1,2 으로 타임아웃이 잠깐 난 후에 mongos3 으로 이용하였으나 아찔했던 상황</li>
        </ul>
      </li>
      <li><strong>how</strong>
        <ul>
          <li>max_pid 값 늘림
            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> /etc/sysctl.d/91-mongos.conf | <span class="nb">grep </span>kernel_pid_max
<span class="nv">kernel_pid_max</span><span class="o">=</span>4194303
</code></pre></div>            </div>
          </li>
        </ul>
      </li>
      <li><strong>result</strong>
        <ul>
          <li>해결됨</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />
<h3 id="application-로직-최적화"><strong>application 로직 최적화</strong></h3>
<ul>
  <li>API 를 목적에 맞게 전부 분리함
    <ul>
      <li>하나의 api 가 여러 목적에 대해 작업을 해주고 있었다. 처음에는 하나의 목적이었으나, uv 가 늘어나면서 점점 목적이 추가가 됐는데, 이로 인해 메트릭을 분석하기가 어려웠다.
        <ol>
          <li>빠른 return 을 보장하는 api</li>
          <li>모든것을 처리하고 return 하는 api</li>
          <li>급한것만 처리하고 return 하는 api</li>
          <li>user 정보를 업데이트하고 return 하는 api</li>
        </ol>

        <p>를 분리하고 목적에 맞게 메트릭을 추가하여 분석하기 시작했다.</p>
      </li>
    </ul>
  </li>
  <li>mongo search 를 최대한으로 줄였음
    <ul>
      <li>딱히 이유가 없고 했었어야 하는 개선인데, 변명해보자면, 모듈화가 꽤 잘 되어있는 api 였기에 몽고 서치가 꽤 자주 일어났다. 말하자면.. msa 구조의 시스템에서 하나의 api 스트림에서 거쳐가는 서버 모두가 유저정보를 필요로 해서 유저서버를 호출하게 되는… 그런… 비슷한 문제라고 생각을 하는데, (netflix 에서는 <a href="http://www.passportjs.org/packages/passport-netflix/" target="_blank">passport</a>를 이용해 문제를 해결했다. 유저 정보를 rest 헤더에 넣어서 계속 사용)</li>
    </ul>
  </li>
  <li>오랜만에 접속한 유저는 많은 수의 insert 가 일어나게 돼서, 이를 줄이기 위해 mongo insert 작업을 병렬처리하였음
    <ul>
      <li>mongodb connection 수와 같은 pool size의 <code class="language-plaintext highlighter-rouge">ThreadPoolTaskExecutor</code> 를 만들고 10k 정도의 큐를 둬서 만에하나 oom 이 나더라도 방지</li>
      <li>queue 가 밀려서 fail 나도 다음에 인서트하면 그만인 작업</li>
    </ul>
    <p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_5.png" class="content-image-1" /><br />
  <em>response max 가 개선됨. p50, p90 는 변화없고 p95 는 10퍼센트정도</em><br />
</p>
  </li>
  <li>redis 캐싱 시간을 늘리기 위해 redis 에 저장되는 데이터를 최소화
    <ul>
      <li><strong>why</strong>
        <ul>
          <li>변동이 적은 것이 확실하다면 캐싱시간을 늘려서 mongodb 부하 감소의 목적</li>
        </ul>
      </li>
      <li><strong>how</strong>
        <ul>
          <li>as-is: 유저별로 A 오브젝트를 레디스 캐싱</li>
          <li>to-be: 유저별로 A 오브젝트의 id 를 레디스 캐싱 + 모든 A 오브젝트 리스트를 로컬캐싱하여 id 로 서치하도록 수정하였음</li>
        </ul>
      </li>
      <li><strong>result</strong>
        <ul>
          <li>cons
            <ul>
              <li>A 오브젝트의 변경이 있을 때 유저별 캐싱이 되어있어 invalidate 가 쉽지 않았기에 캐싱 시간을 늘리는데 한계가 있었으나, 캐싱시간을 늘릴수 있게됨</li>
            </ul>
          </li>
          <li>pros
            <ul>
              <li>해시서치 과정과 로컬캐싱 메모리 추가사용 (별로 안아까움)</li>
              <li>레디스 캐싱타임(2min)에 콜수가 엄청 많아서, 평균 앱 사용시간(~15min)까지 캐싱타임을 늘렸으나 사실상 거의 효과가 없었음.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>cache stampede
    <ul>
      <li><strong>why</strong>
        <ul>
          <li>한 유저가 한 번 앱을 실행하면 서버의 여러 api 를 asynchronous 하게 호출하게 되는데, 각각의 api 들이 전부 이 api 를 호출해서 캐싱이 되어있지 않은 순간에 동시에 요청이 와서 cache stampede 가 종종 발생했다.</li>
        </ul>
      </li>
      <li><strong>how</strong>
        <ul>
          <li>정합성이 크게 문제되는 부분이 아니라 간단하게 단일 redis lock 을 잡고 ~10ms 쓰레드락 시킨 후에 다시 캐시히트를 받아내게 했다.</li>
        </ul>
      </li>
      <li><strong>result</strong>
        <ul>
          <li>응답시간에는 거의 변화가 없었다. 스퀴징중이 아니었기에 cpu 사용률이 줄거나 혹은 쓰레드 슬립때문에 톰캣 리퀘스트 쓰레드가 늘어나는 지도 확인하지 못했다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />
<h3 id="gzip"><strong>GZIP</strong></h3>

:ET
I"<p>많은 서버에서 여러가지 이유로 중복해서 호출이 일어나는 api 가 있었다. 사실 그러라고 만들어 놓은 api 이기도 했지만, data center squize 테스트를 위해 트래픽을 한 쪽에 몰아 넣은 상황에서 uv 가 50k 정도 되었을 때 문제가 발생했다. 머신난이었던 때 인스턴스를 너무 많이 띄우는 괴물 아니냐는 말과 함께 로직에 최적화가 되어있는게 맞는 것인지 하는 챌린지가 많이 들어왔었다. 그래서, 다방면으로 이유를 분석해서 수정을 진행했다. 효과가 좋았던 것들을 몇개 정리하기로 했다.</p>

<p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_1.png" class="content-image-1" /><br />
  <em>tps 15k</em><br />
</p>

<hr />

<h3 id="redis-cache-사이즈-줄이기"><strong>redis cache 사이즈 줄이기</strong></h3>
<ul>
  <li><strong>why</strong>
    <ul>
      <li>mongo search 속도가 느려 레디스 캐싱을 해두었는데, redis 쪽으로 네트워크 bandwidth가 너무 컸다. 네크워크 비용도 아깝고 언젠가는 문제가 됐을 수 있어서 수정했다.</li>
    </ul>
  </li>
  <li><strong>how</strong>
    <ol>
      <li>시리얼라이저를 바꿈
        <ul>
          <li>레디스 넣기 전에 snappy 를 사용했는데, gzip 으로 바꿨음 <a href="https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats" target="_blank">snappy vs gzip</a></li>
        </ul>
      </li>
      <li>캐싱 데이터를 바꿈
        <ul>
          <li>특정 작업의 결과물을 캐싱하던 것에서, 사용되는 파라미터 중 변동되는 것만 캐싱하고 서버가 작업을 매번 새로 하게 함</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>result</strong>
    <ol>
      <li>두 작업 모두 cpu 를 손해보고 network 를 이득보는 작업이라, 효과는 확실했다. 레디스 인아웃은 <code class="language-plaintext highlighter-rouge">500 MiB/s -&gt; 95 MiB/s</code> 로 감소했지만, api 응답은 p50 <code class="language-plaintext highlighter-rouge">7.5ms -&gt; 12ms</code>, p95 <code class="language-plaintext highlighter-rouge">28ms -&gt; 32ms</code> 로 증가했다.</li>
    </ol>
  </li>
</ul>

<p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_2.png" class="content-image-1" /><br />
  <em>레디스 네트워크 in/out. 첫 드랍은 snappy -&gt; gzip, 두번째 드랍은 캐싱데이터 수정</em><br />
</p>
<p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_3.png" class="content-image-1" /><br />
  <em>수정 후 전날과 응답속도 비교. 연산작업의 최적화가 필요해보인다.</em><br />
</p>

<hr />
<h3 id="mongos-이슈들"><strong>mongos 이슈들</strong></h3>

<ul>
  <li>
    <p>mongos command max response 가 이상한 주기성을 가지고 늘어났음</p>

    <p align="center">
  <br /><img alt="img-name" src="/assets/images/backend/apioptimize_4.png" class="content-image-1" /><br />
  <em>uv 가 높은 상황에서, mongos 별로 주기적으로 max response 가 증가하는 피크가 관측됨</em><br />
</p>

    <ul>
      <li><strong>why</strong>
        <ul>
          <li>mongos 로그 중 <code class="language-plaintext highlighter-rouge">LockBusy: could not acquire collection lock for ${dbName}.${collectionName} to split chunk</code>를 발견 <a href="https://jira.mongodb.org/browse/SERVER-56654" target="_blank">관련 이슈</a></li>
          <li>해당 command 가 날아가는 collection 은 5.7b 개의 다큐먼트를 들고 있고, 인서트도 활발히 일어나는 컬렉션이었다. insert 후에 chunk split 하던 와중 분산락을 획득하는 과정이 있는데, 이게 이슈가 있다고 리포트가 되었음</li>
        </ul>
      </li>
      <li><strong>how</strong>
        <ul>
          <li>mongos 버전을 업데이트하는 것으로 해결.</li>
        </ul>
      </li>
      <li><strong>result</strong>
        <ul>
          <li>해결됨. 그러나…</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>3개의 mongos 중 두개가 갑자기 죽었음</p>

    <ul>
      <li><strong>why</strong>
        <ul>
          <li>mongos 버전을 업데이트 하면서, 메모리 증설을 위해 dc 타겟을 바꾸고 호스트를 변경했었는데, 카나리 배포과정에서 몽고 커넥션 수가 호스트의 pid_max 값을 초과했음</li>
          <li>두개의 mongos 의 pid_max 값이 리눅스 system default 값이었고, (32768) 기존 mongos 는 4백만 정도로 세팅을 해서 사용중이었다.
            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@32899a32399f:/# <span class="nb">cat</span> /proc/sys/kernel/pid_max
32768
</code></pre></div>            </div>
          </li>
        </ul>
      </li>
      <li><strong>how</strong>
        <ul>
          <li>max_pid 값 늘림</li>
        </ul>
      </li>
      <li><strong>result</strong>
        <ul>
          <li>해결됨</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />
<h3 id="application-로직-최적화"><strong>application 로직 최적화</strong></h3>
:ET